# 모델과 메모리

LLM: LAG에서 두뇌 역할

![](https://milvus.io/docs/v2.5.x/assets/advanced_rag/self_reflection.png)

## RAG 파이프라인 종류

사전 처리 작업 : 1단계 문서 로드 -> 2단계 텍스트 분할 -> 3단계 임베딩 -> 4단계 벡터 스토어

후반 작업 : 5단계 리트리버의 믄서 검색 -> 6단계 프롬프트 생성 -> 7단계 LLM 답변 생성(GPT에 전달) -> 8단계 체인 생성

## LLM 답변 캐싱하기

캐싱이란 동일한 질문이 발생하면 그에 대한 답변을 별도의 공간에 저장해 두는 것을 말함.

-> 반복 질문에 대한 비용 감소

### 애플리케이션 캐싱 방식

1. 인메모리 캐시
   : 메모리 공간을 활용해 동일한 질문에 대한 답변을 일시적으로 저장. 모델에 다시 요청하지 않아 속도가 빠르고 비용이 절감.
   프로그램 종료시 캐시 사라짐
2. SQLite 캐시
   : 데이터베이스 파일 활용하여 캐시를 저장함. 프로그램을 종료했다가 다시 시작해도 캐시 정보를 유지할 수 있음.

### 직렬화와 역질렬화

- 직렬화는 데이터 구조나 객체의 상태를 저장하거나 전송하기 위해 일련의 바이트나 문자열 형식으로 변환하는 과정

우리가 만든 체인을 저장해야 할 때 특정한 파일 확장자로 지정하기 어려워, 체인을 직렬화하여 JSON 형식으로 변환하여 저장

(모든 데이터 타입이 직렬화가 가능한 것은 아님)

- 역직렬화: 직렬화된 데이터를 객체나 구조의 형태로 복원하는 과정

## Pickle 타입

: 파이썬 객체를 바이너리형태로 변환하는 포멧

## 메모리

ChapGPT 모델 자체는 대화 내용을 기억하지는 않는다. 따라서 이전 대화 내용을 기록하는 메모리 기능을 별도로 구현해야 한다.

메모리 말고도 챗 히스토리라는 기능이 있다.

- 탄: 질문과 답변의 쌍 하나

> 대화는 크게 싱글턴 대화와 멀티턴 대화로 나눌 수 있다.
> 싱글 턴 대화: 한 번의 질문과 답변으로 끝나는 형태. AI가 매번 새로운 질문을 처리하지만, 이전 질문을 참고하지는 않음
> 멀티 턴 대화: 사용자의 여러차례 질문과 AI의 답변이 연속적으로 이어지는 방식. 이전 대화 맥락을 유지하는 것이 중요.

### 대화 버퍼 메모리

: 가장 기본적인 메모리 유형으로, 메시지를 저장한 후 변수를 이용하여 메시지를 추출할 수 있다.

### 대화 버퍼 윈도우 메모리/ 대화 토큰 버퍼 메모리

- 윈도우: 메모리에 저장할 최근 대화의 최대 메시지 수

### 대화 엔티티 메모리

: 엔티티를 추출해 이를 기준으로 저장할 대화의 양을 지정함.

- 엔티티: 대화나 데이터에서 특정한 의미를 가지는 핵심 정보

## 대화 지식 그래프 메모리

- 지식그래프: 객체 간 연결고리를 파악해서 저장
